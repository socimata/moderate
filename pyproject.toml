[project]
name = "moderate"
version = "0"
requires-python = ">=3.12"
dependencies = [
    "diskcache2~=1.0.0",
    "fastapi~=0.116.1",
    "hmr~=0.6.3",
    "hmr-daemon~=0.4.0",
    "httpx[http2]~=0.28.1",
    "openai~=1.108.0",
    "orjson~=3.11.0",
    "promplate~=0.3.5.4",
    "pydantic~=2.11.7",
    "ruff~=0.13.0",
    "typer-slim~=0.19.1",
    "uvicorn-hmr~=0.0.9.0",
]

[tool.ruff]
line-length = 320

[tool.ruff.lint]
extend-select = [
    "I",    # isort
    "N",    # pep8-naming
    "W",    # pycodestyle
    "UP",   # pyupgrade
    "RUF",  # ruff
    "FURB", # refurb
    "C4",   # flake8-comprehensions
    "ARG",  # flake8-unused-arguments
    "PIE",  # flake8-pie
    "PTH",  # flake8-use-pathlib
    "RSE",  # flake8-raise
    "SIM",  # flake8-simplify
    "SLF",  # flake8-self
]

[tool.pyright]
typeCheckingMode = "standard"

[tool.m.aliases]
dev = "uvicorn-hmr gateway.server:app --clear"

[tool.m.chat]
openai_base_url = "http://localhost:8000/demo"
openai_api_key = "socimata-moderate"
options = { model = "llama-3.3-70b" }
